import logging
import os
import asyncio
import nest_asyncio
nest_asyncio.apply()

from telegram import Update
from telegram.ext import ApplicationBuilder, ContextTypes, CommandHandler, MessageHandler, filters
import google.generativeai as genai
import edge_tts
from langdetect import detect

# --- C·∫§U H√åNH ---
TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN")
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)

# --- K·∫æT N·ªêI 2 B·ªò N√ÉO (FLASH & PRO) ---
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
    
    # 1. N√£o Nhanh (Flash) - D√πng ƒë·ªÉ chat th∆∞·ªùng
    model_flash = genai.GenerativeModel('gemini-2.5-flash')
    
    # 2. N√£o Kh·ªßng (Pro) - D√πng khi g√µ /g (Hi·ªán t·∫°i Google ch∆∞a c√≥ 2.5, d√πng 1.5 Pro l√† m·∫°nh nh·∫•t)
    # N·∫øu sau n√†y c√≥ 2.5, ch·ªã ch·ªâ c·∫ßn s·ª≠a t√™n ·ªü ƒë√¢y
    model_pro = genai.GenerativeModel('gemini-2.5-pro') 
else:
    print("‚ö†Ô∏è C·∫¢NH B√ÅO: Ch∆∞a th·∫•y GOOGLE_API_KEY!")

# L∆∞u l·ªãch s·ª≠ chat (Ch·ªâ d√πng cho Flash ƒë·ªÉ ti·∫øt ki·ªám nh·ªõ)
chat_history = {}

# --- C·∫§U H√åNH GI·ªåNG ƒê·ªåC ---
VOICE_MAPPING = {
    'vi': 'vi-VN-NamMinhNeural',       
    'en': 'en-US-ChristopherNeural',   
    'zh': 'zh-CN-YunjianNeural',      
    'default': 'vi-VN-NamMinhNeural'   
}

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    msg = """
    Ch√†o Ch·ªã H·∫°nh! Em l√† VietMaiAI2.0 (Dual-Core).
    
    ‚ö° **Ch·∫ø ƒë·ªô th∆∞·ªùng:** Chat t·ª± nhi√™n (D√πng Flash - Nhanh).
    üß† **Ch·∫ø ƒë·ªô Chuy√™n gia:** G√µ `/g <c√¢u h·ªèi>` ƒë·ªÉ ph√¢n t√≠ch s√¢u (D√πng Pro).
    V√≠ d·ª•: `/g Ph√¢n t√≠ch t√¢m l√Ω h·ªçc trong gi·∫•c m∆°`
    """
    await update.message.reply_text(msg)

async def chat_with_ai(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not GOOGLE_API_KEY:
        await update.message.reply_text("‚ùå L·ªói Key Render!")
        return

    user_text = update.message.text
    chat_id = update.effective_chat.id
    
    print(f"üì© Nh·∫≠n tin: {user_text}") 
    await context.bot.send_chat_action(chat_id=chat_id, action='typing')

    try:
        ai_reply = ""
        is_pro_mode = False

        # --- LOGIC ƒê·ªäNH TUY·∫æN (ROUTING) ---
        
        # TR∆Ø·ªúNG H·ª¢P 1: D√ôNG PRO (N·∫øu c√≥ l·ªánh /g)
        if user_text.lower().startswith("/g "):
            is_pro_mode = True
            # C·∫Øt b·ªè ch·ªØ "/g " ·ªü ƒë·∫ßu
            real_prompt = user_text[3:].strip()
            
            await update.message.reply_text("üß† ƒêang b·∫≠t ch·∫ø ƒë·ªô Chuy√™n gia (Pro)... Ch·ªã ƒë·ª£i ch√∫t nh√©.")
            await context.bot.send_chat_action(chat_id=chat_id, action='typing')
            
            # G·ªçi Model Pro (Kh√¥ng d√πng l·ªãch s·ª≠ ƒë·ªÉ t·∫≠p trung v√†o c√¢u h·ªèi n√†y)
            response = model_pro.generate_content(real_prompt)
            ai_reply = f"ü¶Å **[PRO ANALYSIS]**\n{response.text}"

        # TR∆Ø·ªúNG H·ª¢P 2: D√ôNG FLASH (Chat th∆∞·ªùng)
        else:
            # Qu·∫£n l√Ω l·ªãch s·ª≠ chat cho Flash
            if chat_id not in chat_history:
                chat_history[chat_id] = model_flash.start_chat(history=[
                    {"role": "user", "parts": "B·∫°n l√† tr·ª£ l√Ω th√¢n thi·ªán, tr·∫£ l·ªùi ng·∫Øn g·ªçn, t√¨nh c·∫£m."},
                    {"role": "model", "parts": "D·∫°, em ch√†o Ch·ªã H·∫°nh ·∫°!"}
                ])
            chat = chat_history[chat_id]
            
            response = chat.send_message(user_text)
            ai_reply = response.text

        # --- G·ª¨I K·∫æT QU·∫¢ ---
        
        # 1. G·ª≠i Text
        # N·∫øu d√†i qu√° th√¨ chia nh·ªè tin nh·∫Øn (Telegram gi·ªõi h·∫°n 4096 k√Ω t·ª±)
        if len(ai_reply) > 4000:
            for x in range(0, len(ai_reply), 4000):
                await update.message.reply_text(ai_reply[x:x+4000])
        else:
            await update.message.reply_text(ai_reply)
        
        # 2. T·∫°o Gi·ªçng n√≥i (Ch·ªâ t·∫°o n·∫øu vƒÉn b·∫£n ng·∫Øn < 1000 k√Ω t·ª± ƒë·ªÉ ƒë·ª° spam voice)
        # Pro th∆∞·ªùng tr·∫£ l·ªùi r·∫•t d√†i n√™n ta h·∫°n ch·∫ø ƒë·ªçc voice c·ªßa Pro tr·ª´ khi ng·∫Øn
        if len(ai_reply) < 1000:
            await context.bot.send_chat_action(chat_id=chat_id, action='record_audio')
            
            try:
                # B·ªè c√°i prefix "[PRO]" ra tr∆∞·ªõc khi ƒë·ªçc cho ƒë·ª° k·ª≥
                text_to_speak = ai_reply.replace("ü¶Å **[PRO ANALYSIS]**", "").strip()
                
                lang_code = detect(text_to_speak)
            except: lang_code = 'vi'
            
            short_lang = lang_code.split('-')[0]
            voice = VOICE_MAPPING.get(short_lang, VOICE_MAPPING['default'])
            if short_lang == 'zh': voice = VOICE_MAPPING['zh']

            audio_file = f"voice_{chat_id}.mp3"
            communicate = edge_tts.Communicate(text_to_speak, voice)
            await communicate.save(audio_file)
            
            await update.message.reply_voice(voice=open(audio_file, "rb"))
            
            if os.path.exists(audio_file):
                os.remove(audio_file)
            
    except Exception as e:
        print(f"L·ªói: {e}")
        await update.message.reply_text(f"‚ö†Ô∏è Bot g·∫∑p ch√∫t tr·ª•c tr·∫∑c: {str(e)}")

# --- CH·∫†Y BOT ---
if __name__ == '__main__':
    if not TELEGRAM_TOKEN:
        print("‚ùå L·ªñI: Ch∆∞a c√≥ TELEGRAM_TOKEN!")
    else:
        print("üöÄ VietMaiAI2.0 (Dual-Core) ƒëang kh·ªüi ƒë·ªông...")
        application = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
        application.add_handler(CommandHandler('start', start))
        # X·ª≠ l√Ω m·ªçi tin nh·∫Øn vƒÉn b·∫£n (bao g·ªìm c·∫£ /g v√¨ n√≥ l√† text)
        application.add_handler(MessageHandler(filters.TEXT & (~filters.COMMAND), chat_with_ai))
        application.run_polling()
